{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "The presented code is not optimized, it serves an educational purpose. It is written for CPU, it uses only fully-connected networks and an extremely simplistic dataset. However, it contains all components that can help to understand how RealNVP works, and it should be rather easy to extend it to more sophisticated models. This code could be run almost on any laptop/PC, and it takes a couple of minutes top to get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we go wild and use a dataset that is simpler than MNIST! We use a scipy dataset called Digits. It consists of ~1500 images of size 8x8, and each pixel can take values in $\\{0, 1, \\ldots, 16\\}$.\n",
    "\n",
    "The goal of using this dataset is that everyone can run it on a laptop, without any gpu etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        elif mode == 'val':\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealNVP code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the blogpost for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_orthogonal = lambda shape: nn.init.orthogonal_((shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    \n",
    "    def __init__(self, nets, nett, num_flows, prior, D=2, dequantization=True):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        print('RealNVP by JT.')\n",
    "        \n",
    "        self.dequantization = dequantization\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(num_flows)])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(num_flows)])\n",
    "        # TODO Q3: add orthogonal matrix\n",
    "        orthogonal = lambda shape: nn.init.orthogonal_(torch.empty(shape))\n",
    "        self.r = nn.ParameterList([nn.Parameter(orthogonal((D,D))) for _ in range(num_flows)])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "         \n",
    "        \n",
    "        self.num_flows = num_flows\n",
    "        \n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        # x: input, either images (for the first transformation) or outputs from the previous transformation\n",
    "        # index: it determines the index of the transformation\n",
    "        # forward: whether it is a pass from x to y (forward=True), or from y to x (forward=False)\n",
    "        \n",
    "        # TODO: Q1\n",
    "        (xa, xb) = torch.chunk(x, 2, 1) # split into 2 parts in dim=1\n",
    "        # TODO: Q2\n",
    "        # dim_to_compress = torch.arange(x.shape[1])\n",
    "\n",
    "        # xa = x[:, dim_to_compress % 2 == 0]\n",
    "        # xb = x[:, dim_to_compress % 2 == 1]\n",
    "\n",
    "        \n",
    "        s = self.s[index](xa)\n",
    "        t = self.t[index](xa)\n",
    "        \n",
    "        if forward:\n",
    "            #yb = f^{-1}(x)\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            #xb = f(y)\n",
    "            yb = torch.exp(s) * xb + t\n",
    "        \n",
    "        return torch.cat((xa, yb), 1), s # recombine two parts along dim =1, return scale\n",
    "\n",
    "    def permute(self, x):\n",
    "        \n",
    "        return x.flip(1)\n",
    "    def permute(self,x,index,forward = True):\n",
    "        # TODO: Q3\n",
    "        if forward:\n",
    "            #print(x.shape)\n",
    "            #print(self.r[index].shape)\n",
    "            x = x @ self.r[index]\n",
    "            r = torch.det(self.r[index]).abs().log()\n",
    "        else:\n",
    "            x = x @ self.r[index].inverse()\n",
    "            r = -torch.det(self.r[index]).abs().log()\n",
    "        return torch.tensor(x),r\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in range(self.num_flows):\n",
    "            z, s = self.coupling(z, i, forward=True)\n",
    "            #z = self.permute(z)\n",
    "            z, r = self.permute(z,i,forward=True)\n",
    "            #log_det_J = log_det_J - s.sum(dim=1)\n",
    "            log_det_J = log_det_J + r - s.sum(dim=1)\n",
    "             \n",
    "\n",
    "        return z, log_det_J\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            #x = self.permute(x)\n",
    "            x, _ = self.permute(x,i,forward =False)\n",
    "            x, _ = self.coupling(x, i, forward=False)\n",
    "            \n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z, log_det_J = self.f(x)\n",
    "        if reduction == 'sum':\n",
    "            return -(self.prior.log_prob(z) + log_det_J).sum()\n",
    "        else:\n",
    "            return -(self.prior.log_prob(z) + log_det_J).mean()\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        z = self.prior.sample((batchSize, self.D)) #每一个维度的值有相关性，考虑prior是多元分布\n",
    "        z = z[:, 0, :] # 对z进行操作，抛弃了其中一个维度\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(-1, self.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions: training, evaluation, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather self-explanatory, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        if hasattr(model, 'dequantization'):\n",
    "            if model.dequantization:\n",
    "                test_batch = test_batch + (1. - torch.rand(test_batch.shape))/2.\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = next(iter(test_loader)).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + (1. - torch.rand(batch.shape))/2.\n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "result_dir = 'results/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'realnvp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64   # input dimension\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 100 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RealNVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealNVP by JT.\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 256]           8,448           8,448\n",
      "       LeakyReLU-2            [1, 256]               0               0\n",
      "          Linear-3            [1, 256]          65,792          65,792\n",
      "       LeakyReLU-4            [1, 256]               0               0\n",
      "          Linear-5             [1, 32]           8,224           8,224\n",
      "            Tanh-6             [1, 32]               0               0\n",
      "          Linear-7            [1, 256]           8,448           8,448\n",
      "       LeakyReLU-8            [1, 256]               0               0\n",
      "          Linear-9            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-10            [1, 256]               0               0\n",
      "         Linear-11             [1, 32]           8,224           8,224\n",
      "         Linear-12            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-13            [1, 256]               0               0\n",
      "         Linear-14            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-15            [1, 256]               0               0\n",
      "         Linear-16             [1, 32]           8,224           8,224\n",
      "           Tanh-17             [1, 32]               0               0\n",
      "         Linear-18            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-19            [1, 256]               0               0\n",
      "         Linear-20            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-21            [1, 256]               0               0\n",
      "         Linear-22             [1, 32]           8,224           8,224\n",
      "         Linear-23            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-24            [1, 256]               0               0\n",
      "         Linear-25            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-26            [1, 256]               0               0\n",
      "         Linear-27             [1, 32]           8,224           8,224\n",
      "           Tanh-28             [1, 32]               0               0\n",
      "         Linear-29            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-30            [1, 256]               0               0\n",
      "         Linear-31            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-32            [1, 256]               0               0\n",
      "         Linear-33             [1, 32]           8,224           8,224\n",
      "         Linear-34            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-35            [1, 256]               0               0\n",
      "         Linear-36            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-37            [1, 256]               0               0\n",
      "         Linear-38             [1, 32]           8,224           8,224\n",
      "           Tanh-39             [1, 32]               0               0\n",
      "         Linear-40            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-41            [1, 256]               0               0\n",
      "         Linear-42            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-43            [1, 256]               0               0\n",
      "         Linear-44             [1, 32]           8,224           8,224\n",
      "         Linear-45            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-46            [1, 256]               0               0\n",
      "         Linear-47            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-48            [1, 256]               0               0\n",
      "         Linear-49             [1, 32]           8,224           8,224\n",
      "           Tanh-50             [1, 32]               0               0\n",
      "         Linear-51            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-52            [1, 256]               0               0\n",
      "         Linear-53            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-54            [1, 256]               0               0\n",
      "         Linear-55             [1, 32]           8,224           8,224\n",
      "         Linear-56            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-57            [1, 256]               0               0\n",
      "         Linear-58            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-59            [1, 256]               0               0\n",
      "         Linear-60             [1, 32]           8,224           8,224\n",
      "           Tanh-61             [1, 32]               0               0\n",
      "         Linear-62            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-63            [1, 256]               0               0\n",
      "         Linear-64            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-65            [1, 256]               0               0\n",
      "         Linear-66             [1, 32]           8,224           8,224\n",
      "         Linear-67            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-68            [1, 256]               0               0\n",
      "         Linear-69            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-70            [1, 256]               0               0\n",
      "         Linear-71             [1, 32]           8,224           8,224\n",
      "           Tanh-72             [1, 32]               0               0\n",
      "         Linear-73            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-74            [1, 256]               0               0\n",
      "         Linear-75            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-76            [1, 256]               0               0\n",
      "         Linear-77             [1, 32]           8,224           8,224\n",
      "         Linear-78            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-79            [1, 256]               0               0\n",
      "         Linear-80            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-81            [1, 256]               0               0\n",
      "         Linear-82             [1, 32]           8,224           8,224\n",
      "           Tanh-83             [1, 32]               0               0\n",
      "         Linear-84            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-85            [1, 256]               0               0\n",
      "         Linear-86            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-87            [1, 256]               0               0\n",
      "         Linear-88             [1, 32]           8,224           8,224\n",
      "=======================================================================\n",
      "Total params: 1,319,424\n",
      "Trainable params: 1,319,424\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tg/rbxgd9fx01d5hxbkpmsmj67w0000gn/T/ipykernel_91387/1835179584.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x),r\n"
     ]
    }
   ],
   "source": [
    "# The number of invertible transformations\n",
    "num_flows = 8\n",
    "\n",
    "# scale (s) network i.e.variance\n",
    "nets = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "# translation (t) network i.e. mean\n",
    "nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2))\n",
    "\n",
    "# Prior (a.k.a. the base distribution): Gaussian # init with independence as covariance = 0\n",
    "prior = torch.distributions.MultivariateNormal(torch.zeros(D), torch.eye(D))\n",
    "# Init RealNVP\n",
    "model = RealNVP(nets, nett, num_flows, prior, D=D, dequantization=True)\n",
    "# Print the summary (like in Keras)\n",
    "print(summary(model, torch.zeros(1, 64), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play! Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tg/rbxgd9fx01d5hxbkpmsmj67w0000gn/T/ipykernel_91387/1835179584.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x),r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=51589.00392857143\n",
      "saved!\n",
      "Epoch: 1, val nll=51479.75357142857\n",
      "saved!\n",
      "Epoch: 2, val nll=51552.39035714286\n",
      "Epoch: 3, val nll=51609.38214285715\n",
      "Epoch: 4, val nll=51304.698214285716\n",
      "saved!\n",
      "Epoch: 5, val nll=51311.54142857143\n",
      "Epoch: 6, val nll=51418.43214285714\n",
      "Epoch: 7, val nll=51229.300357142856\n",
      "saved!\n",
      "Epoch: 8, val nll=51450.52\n",
      "Epoch: 9, val nll=51216.2325\n",
      "saved!\n",
      "Epoch: 10, val nll=51401.080714285716\n",
      "Epoch: 11, val nll=51505.03785714286\n",
      "Epoch: 12, val nll=51272.169642857145\n",
      "Epoch: 13, val nll=51479.745357142856\n",
      "Epoch: 14, val nll=51486.96214285714\n",
      "Epoch: 15, val nll=51484.325\n",
      "Epoch: 16, val nll=51643.96\n",
      "Epoch: 17, val nll=51266.58142857143\n",
      "Epoch: 18, val nll=51389.19857142857\n",
      "Epoch: 19, val nll=51286.30071428572\n",
      "Epoch: 20, val nll=51222.05357142857\n",
      "Epoch: 21, val nll=51428.01714285714\n",
      "Epoch: 22, val nll=51447.88285714286\n",
      "Epoch: 23, val nll=51374.99964285714\n",
      "Epoch: 24, val nll=51405.13857142857\n",
      "Epoch: 25, val nll=51299.24464285714\n",
      "Epoch: 26, val nll=51487.44857142857\n",
      "Epoch: 27, val nll=51530.84678571428\n",
      "Epoch: 28, val nll=51340.224285714285\n",
      "Epoch: 29, val nll=51159.768214285716\n",
      "saved!\n",
      "Epoch: 30, val nll=51438.44464285715\n",
      "Epoch: 31, val nll=51220.592142857146\n",
      "Epoch: 32, val nll=51499.239642857145\n",
      "Epoch: 33, val nll=51361.514285714286\n",
      "Epoch: 34, val nll=51416.904285714285\n",
      "Epoch: 35, val nll=51563.2075\n",
      "Epoch: 36, val nll=51550.82892857143\n",
      "Epoch: 37, val nll=51147.639285714286\n",
      "saved!\n",
      "Epoch: 38, val nll=51224.57535714286\n",
      "Epoch: 39, val nll=51567.26857142857\n",
      "Epoch: 40, val nll=51505.06821428571\n",
      "Epoch: 41, val nll=51413.03928571429\n",
      "Epoch: 42, val nll=51471.40214285714\n",
      "Epoch: 43, val nll=51257.165\n",
      "Epoch: 44, val nll=51306.27821428572\n",
      "Epoch: 45, val nll=51509.4425\n",
      "Epoch: 46, val nll=51421.126071428575\n",
      "Epoch: 47, val nll=51646.34892857143\n",
      "Epoch: 48, val nll=51401.0025\n",
      "Epoch: 49, val nll=51423.650714285715\n",
      "Epoch: 50, val nll=51389.83714285714\n",
      "Epoch: 51, val nll=51445.31964285715\n",
      "Epoch: 52, val nll=51326.62535714286\n",
      "Epoch: 53, val nll=51262.17071428571\n",
      "Epoch: 54, val nll=51344.198214285716\n",
      "Epoch: 55, val nll=51391.23642857143\n",
      "Epoch: 56, val nll=51356.36678571429\n",
      "Epoch: 57, val nll=51213.64607142857\n",
      "Epoch: 58, val nll=51365.37535714286\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000])\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.a = nn.ParameterList([nn.Parameter(torch.rand(10), requires_grad=True)])\n",
    "\n",
    "\n",
    "m = Model()\n",
    "b = m.a[0].mean()\n",
    "b.backward()\n",
    "\n",
    "print(m.a[0].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-204.3789, -167.3502,  -91.4750,  ...,  -83.6452, -111.3757,\n",
      "           78.8468],\n",
      "        [ 173.0689,  -51.1208,  -49.0440,  ...,   20.9548,  -23.3552,\n",
      "         -124.2947],\n",
      "        [ -19.1093,  274.5138,   20.5407,  ...,   41.8301, -113.7070,\n",
      "         -156.4598],\n",
      "        ...,\n",
      "        [  74.1822,  188.8872,  249.6976,  ..., -143.9566, -245.5500,\n",
      "         -183.8026],\n",
      "        [   9.1228,   17.9584,  190.9710,  ..., -242.4673,   48.2951,\n",
      "           -6.5418],\n",
      "        [ -74.5159,  209.5502, -110.2849,  ..., -143.0859,   24.0496,\n",
      "          -48.6457]])\n"
     ]
    }
   ],
   "source": [
    "print(model.r[1].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_real(result_dir + name, test_loader)\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例张量\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 选择一个维度（例如，维度1）来压缩\n",
    "dim_to_compress = 1\n",
    "\n",
    "\n",
    "\n",
    "# 使用布尔索引来排除奇数值\n",
    "compressed_x = x[:, x[dim_to_compress] % 2 == 0]\n",
    "\n",
    "print(compressed_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.arange(x.shape[1])\n",
    "xa = x[:, tensor % 2 == 0]\n",
    "       #  xb = x[:, x[dim_to_compress] % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个2x2的张量，用于正交矩阵初始化\n",
    "matrix = torch.empty(2, 2)\n",
    "\n",
    "# 使用orthogonal_方法初始化为正交矩阵\n",
    "nn.init.orthogonal_(matrix)\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[1,2],[3,4]])\n",
    "z = y*matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.det(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [initialize_orthogonal((64, 64)) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.det(r[0]).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
